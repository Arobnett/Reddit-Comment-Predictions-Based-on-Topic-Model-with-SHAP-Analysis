{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SHapley Additive exPlanations Model Interpretability Analysis for Top2Vec Natural Language Processing**: Part 2"
      ],
      "metadata": {
        "id": "ZIhClFk4D3qN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **III. Proposed Methods**: continued"
      ],
      "metadata": {
        "id": "K5u8fGWnEBk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.b. Data Cleaning & Preprocessing:**"
      ],
      "metadata": {
        "id": "utOd0RaUeTax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A natural language processing (NLP) pipeline is built for cleaning and preprocessing the text data. Data is cleaned by stripping newlines, tabs, HTML tags, links, whitespaces, accented characters, special characters, stopwords. Data is preprocessed by converting upper to lower case characters, reducing repeated characters and punctuations, expanding contractions, correcting mis-spelled words, lemmatizing the words, and stemming the words. [[3]](https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a)"
      ],
      "metadata": {
        "id": "TSObRNBdG9az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tutorial site reference: \n",
        "* [Cleaning & Preprocessing Text Data by Building NLP Pipeline](https://towardsdatascience.com/cleaning-preprocessing-text-data-by-building-nlp-pipeline-853148add68a)\n",
        "by [Kajal Yadav](https://github.com/techykajal/Data-Pre-processing/blob/main/Text_Preprocessing.ipynb)"
      ],
      "metadata": {
        "id": "JQavyUrPGHRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INSTALLATIONS:**"
      ],
      "metadata": {
        "id": "B7_-SHA87_lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode # Strips accents from characters.\n",
        "!pip install stopwords # Strips stopwords.\n",
        "!pip install autocorrect # Autocorrects.\n",
        "!pip install nltk # Human language toolkit."
      ],
      "metadata": {
        "id": "pORYvr2H977G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae38ded-91c8-4a3b-91e5-8b30be21ed1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (1.3.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stopwords in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.8/dist-packages (2.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORTED LIBRARIES:**"
      ],
      "metadata": {
        "id": "rU6o6EC08B24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C7vXUN54Nen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2778dba-ac9a-48a3-cc5a-f6044b92db93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # To read the csv file to a data frame.\n",
        "import numpy as np\n",
        "\n",
        "# Importing Libraries for data cleaning \n",
        "import unidecode \n",
        "import re \n",
        "import time \n",
        "import nltk \n",
        "import stopwords \n",
        "nltk.download('stopwords') \n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from autocorrect import Speller \n",
        "from bs4 import BeautifulSoup \n",
        "from nltk.corpus import stopwords \n",
        "from nltk import word_tokenize \n",
        "import string "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **UPLOAD DATA:**"
      ],
      "metadata": {
        "id": "DkK1IHg1-A3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_depression_pcs = pd.read_csv ('/depression_pcs_data.csv')\n",
        "df_depression_pcs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Q49wB6w9-DXC",
        "outputId": "7f52ac97-4446-429c-fcae-c4f8671d348c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Documents_from_Depression_or_PCS_Topics  \\\n",
              "0    I just started my OCONUS assignment, and I've ...   \n",
              "1    I was active duty and palace chased. I actuall...   \n",
              "2    Nonner that’s gonna get paidddd after I get ou...   \n",
              "3    That's the craziest thing I've ever heard! Did...   \n",
              "4               How did you end up making that happen?   \n",
              "..                                                 ...   \n",
              "226  •Kicked out of Florida International Universit...   \n",
              "227  Don't answer the phone, wait a couple hours, t...   \n",
              "228  I like my reason but part of it is that I feel...   \n",
              "229          At least he isn't wearing his area badge.   \n",
              "230  THIS. Our officers/SELs make all so many decis...   \n",
              "\n",
              "     UTC_dates_from_Depression_or_PCS_Topics       Topic  \n",
              "0                                 1468399483         pcs  \n",
              "1                                 1468375101  depression  \n",
              "2                                 1585244281         pcs  \n",
              "3                                 1585243449         pcs  \n",
              "4                                 1624191646         pcs  \n",
              "..                                       ...         ...  \n",
              "226                               1641216805         pcs  \n",
              "227                               1641207022         pcs  \n",
              "228                               1578528305  depression  \n",
              "229                               1576095618         pcs  \n",
              "230                               1642434972  depression  \n",
              "\n",
              "[231 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9726a7c0-aced-476d-977f-8d8b8feca052\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Documents_from_Depression_or_PCS_Topics</th>\n",
              "      <th>UTC_dates_from_Depression_or_PCS_Topics</th>\n",
              "      <th>Topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I just started my OCONUS assignment, and I've ...</td>\n",
              "      <td>1468399483</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I was active duty and palace chased. I actuall...</td>\n",
              "      <td>1468375101</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nonner that’s gonna get paidddd after I get ou...</td>\n",
              "      <td>1585244281</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>That's the craziest thing I've ever heard! Did...</td>\n",
              "      <td>1585243449</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How did you end up making that happen?</td>\n",
              "      <td>1624191646</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>•Kicked out of Florida International Universit...</td>\n",
              "      <td>1641216805</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>Don't answer the phone, wait a couple hours, t...</td>\n",
              "      <td>1641207022</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>I like my reason but part of it is that I feel...</td>\n",
              "      <td>1578528305</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>At least he isn't wearing his area badge.</td>\n",
              "      <td>1576095618</td>\n",
              "      <td>pcs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>THIS. Our officers/SELs make all so many decis...</td>\n",
              "      <td>1642434972</td>\n",
              "      <td>depression</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>231 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9726a7c0-aced-476d-977f-8d8b8feca052')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9726a7c0-aced-476d-977f-8d8b8feca052 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9726a7c0-aced-476d-977f-8d8b8feca052');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Pipeline:**"
      ],
      "metadata": {
        "id": "i7ooh5UIJsvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove newlines & Tabs\n",
        "def remove_newlines_tabs(text):\n",
        "    \"\"\"\n",
        "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
        "    \n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
        "        \n",
        "    Example:\n",
        "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
        "    Output : This is her first day at this place. Please, Be nice to her. \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
        "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
        "    return Formatted_text"
      ],
      "metadata": {
        "id": "L7--fREVeTLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip HTML Tags\n",
        "def strip_html_tags(text):\n",
        "    \"\"\" \n",
        "    This function will remove all the occurrences of html tags from the text.\n",
        "    \n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" after removal of html tags.\n",
        "        \n",
        "    Example:\n",
        "    Input : This is a nice place to live. <IMG>\n",
        "    Output : This is a nice place to live.  \n",
        "    \"\"\"\n",
        "    # Initiating BeautifulSoup object soup.\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    # Get all the text other than html tags.\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text"
      ],
      "metadata": {
        "id": "RZsvsQ2Aif7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Links\n",
        "def remove_links(text):\n",
        "    \"\"\"\n",
        "    This function will remove all the occurrences of links.\n",
        "    \n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" after removal of all types of links.\n",
        "        \n",
        "    Example:\n",
        "    Input : To know more about this website: kajalyadav.com  visit: https://kajalyadav.com//Blogs\n",
        "    Output : To know more about this website: visit:     \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Removing all the occurrences of links that starts with https\n",
        "    remove_https = re.sub(r'http\\S+', '', text)\n",
        "    # Remove all the occurrences of text that ends with .com\n",
        "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
        "    return remove_com"
      ],
      "metadata": {
        "id": "9_niEsIlioE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Whitespaces\n",
        "def remove_whitespace(text):\n",
        "    \"\"\" This function will remove \n",
        "        extra whitespaces from the text\n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" after extra whitespaces removed .\n",
        "        \n",
        "    Example:\n",
        "    Input : How   are   you   doing   ?\n",
        "    Output : How are you doing ?     \n",
        "        \n",
        "    \"\"\"\n",
        "    pattern = re.compile(r'\\s+') \n",
        "    Without_whitespace = re.sub(pattern, ' ', text)\n",
        "    # There are some instances where there is no space after '?' & ')', \n",
        "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
        "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "DferarGFiuyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Accented Characters\n",
        "def accented_characters_removal(text):\n",
        "    # this is a docstring\n",
        "    \"\"\"\n",
        "    The function will remove accented characters from the \n",
        "    text contained within the Dataset.\n",
        "       \n",
        "    arguments:\n",
        "        input_text: \"text\" of type \"String\". \n",
        "                    \n",
        "    return:\n",
        "        value: \"text\" with removed accented characters.\n",
        "        \n",
        "    Example:\n",
        "    Input : Málaga, àéêöhello\n",
        "    Output : Malaga, aeeohello    \n",
        "        \n",
        "    \"\"\"\n",
        "    # Remove accented characters from text using unidecode.\n",
        "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
        "    text = unidecode.unidecode(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "gxm1OfwQi6we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for text lowercasing\n",
        "def lower_casing_text(text):\n",
        "    \n",
        "    \"\"\"\n",
        "    The function will convert text into lower case.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "         value: text in lowercase\n",
        "         \n",
        "    Example:\n",
        "    Input : The World is Full of Surprises!\n",
        "    Output : the world is full of surprises!\n",
        "    \n",
        "    \"\"\"\n",
        "    # Convert text to lower case\n",
        "    # lower() - It converts all upperase letter of given string to lowercase.\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "4_XwSERUkFFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Code for removing repeated characters and punctuations\n",
        "\n",
        "def reducing_incorrect_character_repeatation(text):\n",
        "    \"\"\"\n",
        "    This Function will reduce repeatition to two characters \n",
        "    for alphabets and to one character for punctuations.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Finally formatted text with alphabets repeating to \n",
        "        two characters & punctuations limited to one repeatition \n",
        "        \n",
        "    Example:\n",
        "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
        "    Output : Reallyy, Greeaatt !?.;:)\n",
        "    \n",
        "    \"\"\"\n",
        "    # Pattern matching for all case alphabets\n",
        "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
        "    \n",
        "    # Limiting all the  repeatation to two characters.\n",
        "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
        "    \n",
        "    # Pattern matching for all the punctuations that can occur\n",
        "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
        "    \n",
        "    # Limiting punctuations in previously formatted string to only one.\n",
        "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
        "    \n",
        "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
        "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
        "    return Final_Formatted"
      ],
      "metadata": {
        "id": "I1WpLifpkK76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\",\n",
        "}\n",
        "# The code for expanding contraction words\n",
        "def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
        "    \"\"\"expand shortened words to the actual form.\n",
        "       e.g. don't to do not\n",
        "    \n",
        "       arguments:\n",
        "            input_text: \"text\" of type \"String\".\n",
        "         \n",
        "       return:\n",
        "            value: Text with expanded form of shorthened words.\n",
        "        \n",
        "       Example: \n",
        "       Input : ain't, aren't, can't, cause, can't've\n",
        "       Output :  is not, are not, cannot, because, cannot have \n",
        "    \n",
        "     \"\"\"\n",
        "    # Tokenizing text into tokens.\n",
        "    list_Of_tokens = text.split(' ')\n",
        "\n",
        "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
        "    \n",
        "    # Check whether Word is in lidt_Of_tokens or not.\n",
        "    for Word in list_Of_tokens: \n",
        "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
        "         if Word in CONTRACTION_MAP: \n",
        "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
        "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
        "                \n",
        "    # Converting list of tokens to String.\n",
        "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
        "    return String_Of_tokens"
      ],
      "metadata": {
        "id": "bAz75KJYkMdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code for removing special characters\n",
        "def removing_special_characters(text):\n",
        "    \"\"\"Removing all the special characters except the one that is passed within \n",
        "       the regex to match, as they have imp meaning in the text provided.\n",
        "   \n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text with removed special characters that don't require.\n",
        "        \n",
        "    Example: \n",
        "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
        "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
        "    \n",
        "   \"\"\"\n",
        "    # The formatted text after removing not necessary punctuations.\n",
        "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
        "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
        "    return Formatted_Text"
      ],
      "metadata": {
        "id": "oOeYmPAFkcFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code for removing stopwords\n",
        "stoplist = stopwords.words('english') \n",
        "stoplist = set(stoplist)\n",
        "def removing_stopwords(text):\n",
        "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
        "       & they can be remove safely without comprimising meaning of the sentence.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text after omitted all stopwords.\n",
        "        \n",
        "    Example: \n",
        "    Input : This is Kajal from delhi who came here to study.\n",
        "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
        "    \n",
        "   \"\"\"\n",
        "    # repr() function actually gives the precise information about the string\n",
        "    text = repr(text)\n",
        "    # Text without stopwords\n",
        "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
        "    # Convert list of tokens_without_stopwords to String type.\n",
        "    words_string = ' '.join(No_StopWords)    \n",
        "    return words_string"
      ],
      "metadata": {
        "id": "2HHm8L52kgdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code for spelling corrections\n",
        "def spelling_correction(text):\n",
        "    ''' \n",
        "    This function will correct spellings.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text after corrected spellings.\n",
        "        \n",
        "    Example: \n",
        "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
        "    Output : This is Oberoi from Delhi who came here to study.\n",
        "      \n",
        "    \n",
        "    '''\n",
        "    # Check for spellings in English language\n",
        "    spell = Speller(lang='en')\n",
        "    Corrected_text = spell(text)\n",
        "    return Corrected_text"
      ],
      "metadata": {
        "id": "DGh6nwsAlU_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code for lemmatization\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatization(text):\n",
        "    \"\"\"This function converts word to their root words \n",
        "       without explicitely cut down as done in stemming.\n",
        "    \n",
        "    arguments:\n",
        "         input_text: \"text\" of type \"String\".\n",
        "         \n",
        "    return:\n",
        "        value: Text having root words only, no tense form, no plural forms\n",
        "        \n",
        "    Example: \n",
        "    Input : text reduced \n",
        "    Output :  text reduce\n",
        "    \n",
        "   \"\"\"\n",
        "    # Converting words to their root forms\n",
        "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
        "    return lemma"
      ],
      "metadata": {
        "id": "xo-Uz-5LlXgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing main function to merge all the preprocessing steps.\n",
        "def text_preprocessing(text, accented_chars=True, contractions=True, lemmatization = True,\n",
        "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
        "                       lowercase=True, punctuations=True, mis_spell=True,\n",
        "                       remove_html=True, links=True,  special_chars=True,\n",
        "                       stop_words=True):\n",
        "    \"\"\"\n",
        "    This function will preprocess input text and return\n",
        "    the clean text.\n",
        "    \"\"\"\n",
        "        \n",
        "    if newlines_tabs == True: #remove newlines & tabs.\n",
        "        Data = remove_newlines_tabs(text)\n",
        "        \n",
        "    if remove_html == True: #remove html tags\n",
        "        Data = strip_html_tags(Data)\n",
        "        \n",
        "    if links == True: #remove links\n",
        "        Data = remove_links(Data)\n",
        "        \n",
        "    if extra_whitespace == True: #remove extra whitespaces\n",
        "        Data = remove_whitespace(Data)\n",
        "        \n",
        "    if accented_chars == True: #remove accented characters\n",
        "        Data = accented_characters_removal(Data)\n",
        "        \n",
        "    if lowercase == True: #convert all characters to lowercase\n",
        "        Data = lower_casing_text(Data)\n",
        "        \n",
        "    if repeatition == True: #Reduce repeatitions   \n",
        "        Data = reducing_incorrect_character_repeatation(Data)\n",
        "        \n",
        "    if contractions == True: #expand contractions\n",
        "        Data = expand_contractions(Data)\n",
        "    \n",
        "    if punctuations == True: #remove punctuations\n",
        "        Data = removing_special_characters(Data)\n",
        "    \n",
        "    stoplist = stopwords.words('english') \n",
        "    stoplist = set(stoplist)\n",
        "    \n",
        "    if stop_words == True: #Remove stopwords\n",
        "        Data = removing_stopwords(Data)\n",
        "        \n",
        "    spell = Speller(lang='en')\n",
        "    \n",
        "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
        "        Data = spelling_correction(Data)\n",
        "        \n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    # Converts words to lemma form. \n",
        "    if lemmatization == False: # Whether to overwrite existing lemmas. Defaults to `False`.\n",
        "        Data = lemmatization(Data)\n",
        "    \n",
        "           \n",
        "    return Data\n"
      ],
      "metadata": {
        "id": "L1XwOxd2lwDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing for Content\n",
        "List_Content = df_depression_pcs['Documents_from_Depression_or_PCS_Topics'].to_list()\n",
        "Final_Article = []\n",
        "Complete_Content = []\n",
        "for article in List_Content:\n",
        "    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n",
        "    Final_Article.append(Processed_Content)\n",
        "Complete_Content.extend(Final_Article)\n",
        "df_depression_pcs['Processed_comments'] = Complete_Content\n"
      ],
      "metadata": {
        "id": "uOkCVV_coOwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_depression_pcs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "QV1f2nN0yyfO",
        "outputId": "20bf7dc1-a456-46d6-b7c0-f5cef04d8a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Documents_from_Depression_or_PCS_Topics  \\\n",
              "0    I just started my OCONUS assignment, and I've ...   \n",
              "1    I was active duty and palace chased. I actuall...   \n",
              "2    Nonner that’s gonna get paidddd after I get ou...   \n",
              "3    That's the craziest thing I've ever heard! Did...   \n",
              "4               How did you end up making that happen?   \n",
              "..                                                 ...   \n",
              "226  •Kicked out of Florida International Universit...   \n",
              "227  Don't answer the phone, wait a couple hours, t...   \n",
              "228  I like my reason but part of it is that I feel...   \n",
              "229          At least he isn't wearing his area badge.   \n",
              "230  THIS. Our officers/SELs make all so many decis...   \n",
              "\n",
              "     UTC_dates_from_Depression_or_PCS_Topics       Topic  \\\n",
              "0                                 1468399483         pcs   \n",
              "1                                 1468375101  depression   \n",
              "2                                 1585244281         pcs   \n",
              "3                                 1585243449         pcs   \n",
              "4                                 1624191646         pcs   \n",
              "..                                       ...         ...   \n",
              "226                               1641216805         pcs   \n",
              "227                               1641207022         pcs   \n",
              "228                               1578528305  depression   \n",
              "229                               1576095618         pcs   \n",
              "230                               1642434972  depression   \n",
              "\n",
              "                                    Processed_comments  \n",
              "0    ' started bonus assignment , month . want sepa...  \n",
              "1    ' active duty palace chased . actually 30 % co...  \n",
              "2                     'none gon na get paid get lmaz '  \n",
              "3    'that craziest thing ever heard ! sub last tim...  \n",
              "4                           'how end making happen ? '  \n",
              "..                                                 ...  \n",
              "226  `` * kicked florida international university f...  \n",
              "227  'do answer phone , wait couple hours , text ba...  \n",
              "228  ' like reason part feel inadequate tumbling du...  \n",
              "229                   'at least wearing area badge . '  \n",
              "230  'this . officers self make many decisions zero...  \n",
              "\n",
              "[231 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f5dfba1-dfda-4fc2-b9d2-a42dd6ae9aad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Documents_from_Depression_or_PCS_Topics</th>\n",
              "      <th>UTC_dates_from_Depression_or_PCS_Topics</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Processed_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I just started my OCONUS assignment, and I've ...</td>\n",
              "      <td>1468399483</td>\n",
              "      <td>pcs</td>\n",
              "      <td>' started bonus assignment , month . want sepa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I was active duty and palace chased. I actuall...</td>\n",
              "      <td>1468375101</td>\n",
              "      <td>depression</td>\n",
              "      <td>' active duty palace chased . actually 30 % co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nonner that’s gonna get paidddd after I get ou...</td>\n",
              "      <td>1585244281</td>\n",
              "      <td>pcs</td>\n",
              "      <td>'none gon na get paid get lmaz '</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>That's the craziest thing I've ever heard! Did...</td>\n",
              "      <td>1585243449</td>\n",
              "      <td>pcs</td>\n",
              "      <td>'that craziest thing ever heard ! sub last tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How did you end up making that happen?</td>\n",
              "      <td>1624191646</td>\n",
              "      <td>pcs</td>\n",
              "      <td>'how end making happen ? '</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>•Kicked out of Florida International Universit...</td>\n",
              "      <td>1641216805</td>\n",
              "      <td>pcs</td>\n",
              "      <td>`` * kicked florida international university f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>Don't answer the phone, wait a couple hours, t...</td>\n",
              "      <td>1641207022</td>\n",
              "      <td>pcs</td>\n",
              "      <td>'do answer phone , wait couple hours , text ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>I like my reason but part of it is that I feel...</td>\n",
              "      <td>1578528305</td>\n",
              "      <td>depression</td>\n",
              "      <td>' like reason part feel inadequate tumbling du...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>At least he isn't wearing his area badge.</td>\n",
              "      <td>1576095618</td>\n",
              "      <td>pcs</td>\n",
              "      <td>'at least wearing area badge . '</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>THIS. Our officers/SELs make all so many decis...</td>\n",
              "      <td>1642434972</td>\n",
              "      <td>depression</td>\n",
              "      <td>'this . officers self make many decisions zero...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>231 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f5dfba1-dfda-4fc2-b9d2-a42dd6ae9aad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f5dfba1-dfda-4fc2-b9d2-a42dd6ae9aad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f5dfba1-dfda-4fc2-b9d2-a42dd6ae9aad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Cleaned_Depression_PCS_Data = df_depression_pcs.to_csv('Cleaned_Depression_PCS_Data.csv', index = False)"
      ],
      "metadata": {
        "id": "6axIGz-H1Xa2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}