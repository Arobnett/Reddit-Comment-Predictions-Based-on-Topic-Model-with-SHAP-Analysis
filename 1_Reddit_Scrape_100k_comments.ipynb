{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SHapley Additive exPlanations Model Interpretability Analysis for Top2Vec Natural Language Processing**: Part 1"
      ],
      "metadata": {
        "id": "qk4VDa0f1K_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Abstract**"
      ],
      "metadata": {
        "id": "E6YHPDFX1qhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **III.A. Data Sourcing and Management:**"
      ],
      "metadata": {
        "id": "c7xfsW8020N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The data in this study is web scraped from comment conversations in the subreddit called “r/AirForce”. This subreddit is used members of the United States Air Force (USAF) and for Reddit users who are interested the USAF. Reddit users here can post about mutual interests, experiences, and questions among the USAF community as well as express grievances regarding any current events and trends. While branches of the military have analogous subreddit\n",
        "pages, the USAF was chosen for this research project due to my general familiarity from having worked with the Deparment of Air Force (DAF). [[1]](https://www.reddit.com/r/AirForce/)"
      ],
      "metadata": {
        "id": "WNnBZt2K2-QE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.a. How the data was obtained/collected:**"
      ],
      "metadata": {
        "id": "CvHvTld759V_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The subreddit data was scraped from 2015 to 2022 in order to be used for time series, correlation, regression, and model interpretability analyses between the association of \"Permanent Change of Station (PCS)\" topic related comments and \"depression\" topic related comments. About 100,000 comments were scraped for analysis using Pushshift Reddit API. [[2]](https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286)"
      ],
      "metadata": {
        "id": "YFhEAWFg6aX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installations:**"
      ],
      "metadata": {
        "id": "3ZRlKf4A-xeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Retrieval:"
      ],
      "metadata": {
        "id": "bb7C7LFYeqZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main wrapper in Python for Pushshift.\n",
        "pip install pmaw pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au004PJnbqXv",
        "outputId": "51128c82-cc04-4ffb-bad3-ce3589e1bb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pmaw\n",
            "  Using cached pmaw-2.1.3-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting praw\n",
            "  Using cached praw-7.6.1-py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pmaw) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting update-checker>=0.18\n",
            "  Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Using cached websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "Collecting prawcore<3,>=2.1\n",
            "  Using cached prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pmaw) (1.24.3)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw, pmaw\n",
            "Successfully installed pmaw-2.1.3 praw-7.6.1 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Imports and Scrape:**"
      ],
      "metadata": {
        "id": "0AQxhd9I-23q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp6oC4kkayY_",
        "outputId": "de53dfec-645c-4968-dc3f-7fddb48f3169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n",
            "WARNING:pmaw.PushshiftAPIBase:Not all PushShift shards are active. Query results may be incomplete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 5672 comments from Pushshift\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # For data manipulation.\n",
        "from pmaw import PushshiftAPI # For web scrape.\n",
        "api = PushshiftAPI() # Application program interface.\n",
        "\n",
        "import datetime as dt # For date time manipulation.\n",
        "before = int(dt.datetime(2021,01,1,0,0).timestamp()) # End date.\n",
        "after = int(dt.datetime(2015,12,1,0,0).timestamp()) # Beginning date.\n",
        "\n",
        "subreddit = \"AirForce\" # Subreddit location.\n",
        "limit=100000 # Approximate comment document scrape limit.\n",
        "# Scraping process:\n",
        "comments = api.search_comments(subreddit=subreddit, # Subreddit.\n",
        "                               limit=limit, # Data scrape amount.\n",
        "                               before=before, # End date.\n",
        "                               after=after # Beginning date.\n",
        "                               )\n",
        "# Number of comments/rows from the dataset.\n",
        "print(f'Retrieved {len(comments)} comments from Pushshift')\n",
        "\n",
        "comments_df = pd.DataFrame(comments) # Turns dataset to dataframe. \n",
        "# Preview the comments data.\n",
        "comments_df.head(5)\n",
        "# Saves dataframe to csv file.\n",
        "comments_df.to_csv('./AF_comments_100k.csv', header=True, index=False, \n",
        "                   columns=list(comments_df.axes[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example data file generated from Reddit scrape to be used for project analysis:\n",
        "* [AF_comments_100k.csv](https://drive.google.com/file/d/11ROOAKTTEj9djDy1c-ct7I3W2-Jmmby5/view?usp=share_link)"
      ],
      "metadata": {
        "id": "_1TIxGwz0cTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tutorial site reference:\n",
        "* How to Scrape Large Amounts of Reddit Data by Matt Podolak\n",
        "  * https://medium.com/swlh/how-to-scrape-large-amounts-of-reddit-data-using-pushshift-1d33bde9286\n"
      ],
      "metadata": {
        "id": "FSu2ytUdbUi5"
      }
    }
  ]
}